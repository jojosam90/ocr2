{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNnoXMjrkeL+T9z/bCUByvQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojosam90/ocr2/blob/main/streamlit_train_predict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J4lo3CxQENfU"
      },
      "outputs": [],
      "source": [
        "#!pip install streamlit -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "#%%writefile keras_ui_streamlit.py\n",
        "\n",
        "import streamlit as st\n",
        "import train_streamlit as app1\n",
        "import predict_streamlit as app2\n",
        "\n",
        "# Define pages based on apps imported.\n",
        "PAGES = {\n",
        "    \"Train\": app1,\n",
        "    \"Predict\": app2\n",
        "}\n",
        "st.sidebar.title('Navigation')\n",
        "selection = st.sidebar.radio(\"Go to\", list(PAGES.keys()))\n",
        "page = PAGES[selection]\n",
        "page.app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zGEeGH5Emaq",
        "outputId": "80620dd9-aead-4775-f039-a9e0656d9ef1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_streamlit.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import time\n",
        "import streamlit as st\n",
        "\n",
        "def footer_markdown():\n",
        "    footer=\"\"\"\n",
        "    <style>\n",
        "    a:link , a:visited{\n",
        "    color: blue;\n",
        "    background-color: transparent;\n",
        "    text-decoration: underline;\n",
        "    }\n",
        "    \n",
        "    a:hover,  a:active {\n",
        "    color: red;\n",
        "    background-color: transparent;\n",
        "    text-decoration: underline;\n",
        "    }\n",
        "    \n",
        "    .footer {\n",
        "    position: fixed;\n",
        "    left: 0;\n",
        "    bottom: 0;\n",
        "    width: 100%;\n",
        "    background-color: white;\n",
        "    color: black;\n",
        "    text-align: center;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"footer\">\n",
        "    <p>Developed by <a style='display: block; text-align: center;' >Shubhaditya Goswami</a></p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return footer\n",
        "\n",
        "\n",
        "def app():\n",
        "    \"\"\"\n",
        "    Main function that contains the application to train keras based models.\n",
        "    \"\"\"\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        \"\"\"\n",
        "        Tensorflow function to compute gradient, loss and metric defined globally \n",
        "        based on given data and model.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x, training=True)\n",
        "            loss_value = loss_fn(y, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        train_acc_metric.update_state(y, logits)\n",
        "        return loss_value\n",
        "    \n",
        "    \n",
        "    @tf.function\n",
        "    def test_step(x, y):\n",
        "        \"\"\"\n",
        "        Tensorflow function to compute predicted loss and metric using sent \n",
        "        data from the trained model.\n",
        "        \"\"\"\n",
        "        val_logits = model(x, training=False)\n",
        "        val_acc_metric.update_state(y, val_logits)\n",
        "        return loss_fn(y, val_logits)\n",
        "    \n",
        "    st.title(\"Keras Training Basic UI\")\n",
        "    st.header(\"A Streamlit based Web UI To Create And Train Models\")\n",
        "    st.subheader(\"Create Network:\")\n",
        "    \n",
        "    st.markdown(footer_markdown(),unsafe_allow_html=True)\n",
        "    \n",
        "    in_pl = st.empty()\n",
        "    input_shape = in_pl.text_input(\"Enter input shape, either number or tuple\")\n",
        "    if input_shape:\n",
        "        # Check if input shape is in correct format.\n",
        "        input_valid = False\n",
        "        while(not input_valid):\n",
        "            placeholder = st.empty()\n",
        "            # Format input_shape.\n",
        "            if input_shape.isnumeric():\n",
        "                input_shape = int(input_shape)\n",
        "                input_valid = True\n",
        "            elif \"(\" in input_shape:\n",
        "                input_shape = eval(input_shape)\n",
        "                input_valid = True\n",
        "            else:\n",
        "                input_shape = in_pl.text_input(\"Enter input shape, either number or tuple\")\n",
        "                placeholder.write(\"Invalid input shape.\")\n",
        "        \n",
        "            \n",
        "        dense_layer_num = st.number_input(\"Enter number of dense layers\")\n",
        "    \n",
        "        if dense_layer_num:\n",
        "            dense_layer_num = int(dense_layer_num)\n",
        "            dense_layer_node = st.number_input(\"Enter number of nodes in dense layers\")\n",
        "    \n",
        "            if dense_layer_node:\n",
        "                dense_layer_node = int(dense_layer_node)\n",
        "                dense_activation = st.text_input(\"Enter dense layer activation\")\n",
        "                \n",
        "                if dense_activation:\n",
        "                    output_num = st.number_input(\"Enter number of output nodes\")\n",
        "    \n",
        "                    if output_num:\n",
        "                        output_num = int(output_num)\n",
        "                        inputs = keras.Input(shape=(input_shape,), name=\"digits\")\n",
        "                        dense_layer_dict = {}\n",
        "                        for i in range(dense_layer_num):\n",
        "                            if i == 0:\n",
        "                                dense_layer_dict[i]= layers.Dense(dense_layer_node, \n",
        "                                                                  activation=dense_activation)(\n",
        "                                                                      inputs)\n",
        "                            else:\n",
        "                                dense_layer_dict[i] = layers.Dense(dense_layer_node, \n",
        "                                                                   activation=dense_activation)(\n",
        "                                                                       dense_layer_dict[i-1])\n",
        "                        outputs = layers.Dense(output_num, name=\"predictions\")(dense_layer_dict[i])\n",
        "                        model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "                        \n",
        "                        optim_choice = st.radio(\"Choose optimizer\",(\"SGD\",\"Adam\"))\n",
        "                        # Instantiate an optimizer.\n",
        "                        if optim_choice == \"SGD\":\n",
        "                            optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "                        elif optim_choice == \"Adam\":\n",
        "                            optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "                        else:\n",
        "                            optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "                            \n",
        "                        optim_choice = st.radio(\"Choose loss function\",(\"Categorical crossentropy\",\n",
        "                                                                        \"Sparse Categorical crossentropy\"))\n",
        "                        # Instantiate a loss function.\n",
        "                        if optim_choice == \"Categorical crossentropy\":\n",
        "                            loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "                            # Prepare the metrics.\n",
        "                            train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "                            val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "                        elif optim_choice == \"Sparse Categorical crossentropy\":\n",
        "                            loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "                            # Prepare the metrics.\n",
        "                            train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "                            val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "                    \n",
        "                        # Load data.\n",
        "                        optim_choice = st.radio(\"Choose dataset\",(\"MNIST\", \"CIFAR10\", \"CIFAR100\",\n",
        "                                                                  \"IMDB Movie Review\", \"Reuters Newswire\",\n",
        "                                                                  \"Fashion MNIST\",\"Boston Housing\"))\n",
        "                        if optim_choice == \"MNIST\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "                        elif optim_choice == \"CIFAR10\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "                        elif optim_choice == \"CIFAR100\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
        "                        elif optim_choice == \"IMDB Movie Review\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data()\n",
        "                        elif optim_choice == \"Reuters Newswire\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.reuters.load_data()\n",
        "                        elif optim_choice == \"Fashion MNIST\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "                        elif optim_choice == \"Boston Housing\":\n",
        "                            (x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()\n",
        "                        \n",
        "                        # Reshape train and test data.\n",
        "                        # Prepare the training dataset.\n",
        "                        batch_size = st.number_input(\"Enter batch size\")\n",
        "    \n",
        "                        if batch_size:\n",
        "                            batch_size = int(batch_size)\n",
        "                            x_train = np.reshape(x_train, (-1, input_shape))\n",
        "                            x_test = np.reshape(x_test, (-1, input_shape))\n",
        "                            train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "                            train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "                            \n",
        "                            # Calculate number of training steps.\n",
        "                            train_steps_per_epoch = len(x_train) //batch_size\n",
        "                            \n",
        "                            # Prepare the validation dataset.\n",
        "                            # Reserve 10,000 samples for validation.\n",
        "                            val_ratio = st.number_input(\"Enter validation ratio\")\n",
        "    \n",
        "                            if val_ratio:\n",
        "                                val_ratio = float(val_ratio)\n",
        "                                val_size = int(val_ratio * x_train.shape[0])\n",
        "                                x_val = x_train[-val_size:]\n",
        "                                y_val = y_train[-val_size:]\n",
        "                                x_train = x_train[:-val_size]\n",
        "                                y_train = y_train[:-val_size]\n",
        "                                val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "                                val_dataset = val_dataset.batch(batch_size)\n",
        "                                \n",
        "                                epochs = st.number_input(\"Enter number of epochs\")\n",
        "    \n",
        "                                if epochs:\n",
        "                                    epochs = int(epochs)\n",
        "                                    save_model = st.text_input(\"Model name, if want to save model...\")\n",
        "                                    if save_model:\n",
        "                                        save_condition = st.radio(\"Choose save condition...\",\n",
        "                                                                  (\"train acc\",\"val acc\",\"train loss\",\"val loss\"))\n",
        "                                        \n",
        "                                    if st.button(\"Train\"):\n",
        "                                        st.write(\"Starting training with {} epochs...\".format(epochs))\n",
        "                                        # epochs = 2\n",
        "                                        for epoch in range(epochs):\n",
        "                                            print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "                                            st.write(\"Epoch {}\".format(epoch+1))\n",
        "                                            start_time = time.time()\n",
        "                                            progress_bar = st.progress(0.0)\n",
        "                                            percent_complete = 0\n",
        "                                            epoch_time = 0\n",
        "                                            # Creating empty placeholder to update each step result in epoch.\n",
        "                                            st_t = st.empty()\n",
        "                                            \n",
        "                                            train_loss_list = []\n",
        "                                            # Iterate over the batches of the dataset.\n",
        "                                            for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "                                                start_step = time.time()\n",
        "                                                loss_value = train_step(x_batch_train, y_batch_train)\n",
        "                                                end_step = time.time()\n",
        "                                                epoch_time += (end_step - start_step)\n",
        "                                                train_loss_list.append(float(loss_value))\n",
        "                                                \n",
        "                                                # Log every 200 batches.\n",
        "                                                if step % 200 == 0:\n",
        "                                                    print(\n",
        "                                                        \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                                                        % (step, float(loss_value))\n",
        "                                                    )\n",
        "                                                    print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "                                                    step_acc = float(train_acc_metric.result())\n",
        "                                                    percent_complete = ((step/train_steps_per_epoch))\n",
        "                                                    progress_bar.progress(percent_complete)\n",
        "                                                    st_t.write(\"Duration : {0:.2f}s, Training acc. : {1:.4f}\"\\\n",
        "                                                     .format((epoch_time),float(step_acc)))\n",
        "                                            \n",
        "                                            progress_bar.progress(1.0)\n",
        "                                        \n",
        "                                            # Display metrics at the end of each epoch.\n",
        "                                            train_acc = train_acc_metric.result()\n",
        "                                            print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "                                        \n",
        "                                            # Reset training metrics at the end of each epoch\n",
        "                                            train_acc_metric.reset_states()\n",
        "                                            \n",
        "                                            # Find epoch training loss.\n",
        "                                            print(train_loss_list)\n",
        "                                            train_loss = round((sum(train_loss_list)/len(train_loss_list)), 5)\n",
        "                                        \n",
        "                                            val_loss_list = []\n",
        "                                            # Run a validation loop at the end of each epoch.\n",
        "                                            for x_batch_val, y_batch_val in val_dataset: \n",
        "                                                val_loss_list.append(float(test_step(x_batch_val, y_batch_val)))\n",
        "                                            \n",
        "                                            # Find epoch validation loss.\n",
        "                                            val_loss = round((sum(val_loss_list)/len(val_loss_list)), 5)\n",
        "                                        \n",
        "                                            val_acc = val_acc_metric.result()\n",
        "                                            val_acc_metric.reset_states()\n",
        "                                            \n",
        "                                            print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "                                            print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "                                            st_t.write(\"Duration : {0:.2f}s, Training acc. : {1:.4f}, Validation acc.:{2:.4f}\"\\\n",
        "                                                     .format((time.time() - start_time),float(train_acc),float(val_acc)))\n",
        "    \n",
        "                                            \n",
        "                                            # Check if model needs to be saved, and if yess, then with what condition.\n",
        "                                            if save_model:\n",
        "                                                if save_condition:\n",
        "                                                    if epoch == 0:\n",
        "                                                        best_train_acc = train_acc\n",
        "                                                        best_train_loss = train_loss\n",
        "                                                        best_val_loss = val_loss\n",
        "                                                        best_val_acc = val_acc\n",
        "                                                        \n",
        "                                                        # Save first model.\n",
        "                                                        model.save(\"./model/\"+save_model+\".h5\", overwrite = True,\n",
        "                                                                   include_optimizer=True)\n",
        "                                                        if save_condition in (\"train acc\",\"val acc\"):\n",
        "                                                            st.write(\"Saved model {} as {} increased from 0 to {}.\"\\\n",
        "                                                                     .format(save_model+\".h5\", save_condition,\n",
        "                                                                             round(train_acc,3) if save_condition == \"train acc\" else round(val_acc,3)))\n",
        "                                                        else:\n",
        "                                                            st.write(\"Saved model {} as {} decreased from infinite to {}.\"\\\n",
        "                                                                     .format(save_model+\".h5\", save_condition,\n",
        "                                                                             round(train_loss,3) if save_condition == \"train loss\" else round(val_loss,3)))\n",
        "                                                    else:\n",
        "                                                        if save_condition == \"train acc\":\n",
        "                                                            if train_acc >= best_train_acc:\n",
        "                                                                model.save(\"./model/\"+save_model+\".h5\", overwrite = True,\n",
        "                                                                   include_optimizer=True)\n",
        "                                                                st.write(\"Saved model {} as {} increased from {} to {}.\"\\\n",
        "                                                                     .format(save_model+\".h5\", save_condition,\n",
        "                                                                             round(best_train_acc,3),round(train_acc,3)))\n",
        "                                                                best_train_acc = train_acc\n",
        "                                                            else:\n",
        "                                                                st.write(\"Not saving model as {} did not increase from {}.\"\\\n",
        "                                                                     .format(save_condition, round(best_train_acc,3)))\n",
        "                                                        elif save_condition == \"val acc\":\n",
        "                                                            if val_acc >= best_val_acc:\n",
        "                                                                model.save(\"./model/\"+save_model+\".h5\", overwrite = True,\n",
        "                                                                   include_optimizer=True)\n",
        "                                                                st.write(\"Saved model {} as {} increased from {} to {}.\"\\\n",
        "                                                                     .format(save_model+\".h5\", save_condition,\n",
        "                                                                             round(best_val_acc,3),round(val_acc,3)))\n",
        "                                                                best_val_acc = val_acc\n",
        "                                                            else:\n",
        "                                                                st.write(\"Not saving model as {} did not increase from {}.\"\\\n",
        "                                                                     .format(save_condition, round(best_val_acc,3)))\n",
        "                                                                    \n",
        "                                                        elif save_condition == \"train loss\":\n",
        "                                                            if train_loss >= best_train_loss:\n",
        "                                                                model.save(\"./model/\"+save_model+\".h5\", overwrite = True,\n",
        "                                                                   include_optimizer=True)\n",
        "                                                                st.write(\"Saved model {} as {} decreased from {} to {}.\"\\\n",
        "                                                                     .format(save_model+\".h5\", save_condition,\n",
        "                                                                             round(best_train_loss,3),round(train_loss,3)))\n",
        "                                                                best_train_loss = train_loss\n",
        "                                                            else:\n",
        "                                                                st.write(\"Not saving model as {} did not increase from {}.\"\\\n",
        "                                                                     .format(save_condition, round(best_train_loss,3)))\n",
        "                                                                    \n",
        "                                                        elif save_condition == \"val loss\":\n",
        "                                                            if val_loss >= best_val_loss:\n",
        "                                                                model.save(\"./model/\"+save_model+\".h5\", overwrite = True,\n",
        "                                                                   include_optimizer=True)\n",
        "                                                                st.write(\"Saved model {} as {} decreased from {} to {}.\"\\\n",
        "                                                                     .format(save_model+\".h5\", save_condition,\n",
        "                                                                             round(best_val_loss,3),round(val_loss,3)))\n",
        "                                                                best_val_loss = val_loss\n",
        "                                                            else:\n",
        "                                                                st.write(\"Not saving model as {} did not increase from {}.\"\\\n",
        "                                                                     .format(save_condition, round(best_val_loss,3)))\n",
        "                                                                \n",
        "                                                    \n",
        "if __name__=='__main__':\n",
        "    app()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Maef_aMbGh7H",
        "outputId": "482942d7-b0a9-4da9-bf58-0a7dda6b1adb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile predict_streamlit.py\n",
        "\n",
        "import os\n",
        "import streamlit as st \n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from io import StringIO\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def footer_markdown():\n",
        "    footer=\"\"\"\n",
        "    <style>\n",
        "    a:link , a:visited{\n",
        "    color: blue;\n",
        "    background-color: transparent;\n",
        "    text-decoration: underline;\n",
        "    }\n",
        "    \n",
        "    a:hover,  a:active {\n",
        "    color: red;\n",
        "    background-color: transparent;\n",
        "    text-decoration: underline;\n",
        "    }\n",
        "    \n",
        "    .footer {\n",
        "    position: fixed;\n",
        "    left: 0;\n",
        "    bottom: 0;\n",
        "    width: 100%;\n",
        "    background-color: white;\n",
        "    color: black;\n",
        "    text-align: center;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"footer\">\n",
        "    <p>Developed by <a style='display: block; text-align: center;' >Shubhaditya Goswami</a></p>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    return footer\n",
        "\n",
        "\n",
        "def app():\n",
        "    \"\"\"\n",
        "    Main function that contains the application for getting predictions from \n",
        "    keras based trained models.\n",
        "    \"\"\"\n",
        "    # Get list of saved h5 models, which will be displayed in option to load.\n",
        "    h5_file_list = [file for file in os.listdir(\"./model\") if file.endswith(\".h5\")]\n",
        "    h5_file_names = [os.path.splitext(file)[0] for file in h5_file_list]\n",
        "    \n",
        "    st.title(\"Keras Prediction Basic UI\")\n",
        "    st.header(\"A Streamlit based Web UI To Get Predictions From Trained Models\")\n",
        "    st.markdown(footer_markdown(),unsafe_allow_html=True)\n",
        "    model_type = st.radio(\"Choose trained model to load...\", h5_file_names)\n",
        "    \n",
        "    loaded_model = tf.keras.models.load_model(\"./model/{}.h5\".format(model_type))\n",
        "    \n",
        "    uploaded_file = st.file_uploader(\"Choose an image...\", type=\"jpg\")\n",
        "    if uploaded_file is not None:\n",
        "        if \"mnist\" in model_type:\n",
        "            image = Image.open(uploaded_file)\n",
        "            image = image.resize((28,28), Image.NEAREST)\n",
        "            st.image(image, caption='Uploaded Image.', use_column_width=False)\n",
        "            st.write(\"\")\n",
        "            st.write(\"Identifying...\")\n",
        "            # Convert to grayscale if RGB.\n",
        "            print(image.size)\n",
        "            print(image.mode)\n",
        "            if image.mode == \"RGB\":\n",
        "                image = image.convert(\"L\")\n",
        "            # Convert to numpy array and resize.\n",
        "            image = np.array(image)\n",
        "            image = np.resize(image,(1,784))\n",
        "            \n",
        "            # Get prediction.\n",
        "            yhat = loaded_model.predict(image)\n",
        "            # Convert the probabilities to class labels\n",
        "            label = np.argmax(yhat, axis=1)[0]\n",
        "            st.write('%s' % (label) )\n",
        "            \n",
        "\n",
        "if __name__=='__main__':\n",
        "    app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwIMr_3AIV_Q",
        "outputId": "21fc1640-4d12-43ae-cee7-02a18e3f45c9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting predict_streamlit.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyngrok"
      ],
      "metadata": {
        "id": "s8T6gimLEzFo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyngrok import ngrok\n",
        "# url = ngrok.connect(port='8501')\n",
        "# print(url)"
      ],
      "metadata": {
        "id": "3HXe1KueE4Vy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiiOgWwaFAdz",
        "outputId": "4ea8a7bb-5ede-4561-a9bf-fb3adc07ed60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.548s\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.82.18.199:8501\u001b[0m\n",
            "\u001b[0m\n",
            "your url is: https://orange-bikes-speak-34-82-18-199.loca.lt\n",
            "2022-12-11 03:22:08.241 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 564, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 15, in <module>\n",
            "    page.app()\n",
            "  File \"/content/predict_streamlit.py\", line 48, in app\n",
            "    h5_file_list = [file for file in os.listdir(\"./model\") if file.endswith(\".h5\")]\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './model'\n",
            "2022-12-11 03:22:53.996907: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "\n",
            "Start of epoch 0\n",
            "2022-12-11 03:24:16.081 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 564, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 15, in <module>\n",
            "    page.app()\n",
            "  File \"/content/train_streamlit.py\", line 220, in app\n",
            "    loss_value = train_step(x_batch_train, y_batch_train)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_filecnlrwbif.py\", line 17, in tf__train_step\n",
            "    loss_value = ag__.converted_call(ag__.ld(loss_fn), (ag__.ld(y), ag__.ld(logits)), None, fscope)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n",
            "    losses = call_fn(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call\n",
            "    return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n",
            "    return backend.categorical_crossentropy(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n",
            "    target.shape.assert_is_compatible_with(output.shape)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/content/train_streamlit.py\", line 53, in train_step  *\n",
            "        loss_value = loss_fn(y, logits)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__  **\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n",
            "        return backend.categorical_crossentropy(\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n",
            "        target.shape.assert_is_compatible_with(output.shape)\n",
            "\n",
            "    ValueError: Shapes (64,) and (64, 10) are incompatible\n",
            "\n",
            "\n",
            "Start of epoch 0\n",
            "2022-12-11 03:24:20.332 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 564, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 15, in <module>\n",
            "    page.app()\n",
            "  File \"/content/train_streamlit.py\", line 220, in app\n",
            "    loss_value = train_step(x_batch_train, y_batch_train)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_filecnlrwbif.py\", line 17, in tf__train_step\n",
            "    loss_value = ag__.converted_call(ag__.ld(loss_fn), (ag__.ld(y), ag__.ld(logits)), None, fscope)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n",
            "    losses = call_fn(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call\n",
            "    return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n",
            "    return backend.categorical_crossentropy(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n",
            "    target.shape.assert_is_compatible_with(output.shape)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/content/train_streamlit.py\", line 53, in train_step  *\n",
            "        loss_value = loss_fn(y, logits)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__  **\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n",
            "        return backend.categorical_crossentropy(\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n",
            "        target.shape.assert_is_compatible_with(output.shape)\n",
            "\n",
            "    ValueError: Shapes (64,) and (64, 10) are incompatible\n",
            "\n",
            "\n",
            "Start of epoch 0\n",
            "2022-12-11 03:24:33.760 Uncaught app exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 564, in _run_script\n",
            "    exec(code, module.__dict__)\n",
            "  File \"/content/app.py\", line 15, in <module>\n",
            "    page.app()\n",
            "  File \"/content/train_streamlit.py\", line 220, in app\n",
            "    loss_value = train_step(x_batch_train, y_batch_train)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/tensorflow/python/util/traceback_utils.py\", line 153, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"/tmp/__autograph_generated_filecnlrwbif.py\", line 17, in tf__train_step\n",
            "    loss_value = ag__.converted_call(ag__.ld(loss_fn), (ag__.ld(y), ag__.ld(logits)), None, fscope)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__\n",
            "    losses = call_fn(y_true, y_pred)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call\n",
            "    return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n",
            "    return backend.categorical_crossentropy(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n",
            "    target.shape.assert_is_compatible_with(output.shape)\n",
            "ValueError: in user code:\n",
            "\n",
            "    File \"/content/train_streamlit.py\", line 53, in train_step  *\n",
            "        loss_value = loss_fn(y, logits)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 139, in __call__  **\n",
            "        losses = call_fn(y_true, y_pred)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 243, in call  **\n",
            "        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1787, in categorical_crossentropy\n",
            "        return backend.categorical_crossentropy(\n",
            "    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5119, in categorical_crossentropy\n",
            "        target.shape.assert_is_compatible_with(output.shape)\n",
            "\n",
            "    ValueError: Shapes (64,) and (64, 10) are incompatible\n",
            "\n"
          ]
        }
      ]
    }
  ]
}